{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "16fae098-d7f8-4168-86e0-9a1d06c5a849",
   "metadata": {},
   "source": [
    "# 1. The MNIST dataset consists of numeric pictures written cursively from 0 to 9. If you want to create a neural network that classifies the data, what is the activation function of the last fully connected (=Dense) layer?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "372003a0-d16a-49bf-a98d-27f35c071936",
   "metadata": {},
   "source": [
    "#Answer:- \n",
    ". Activation function of the last layer for MNIST classification\n",
    "For multi-class classification (digits 0–9), the last dense layer should have softmax activation, which outputs a probability distribution over the 10 classes."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0ab1dcb-d0ed-4ce0-bbde-793f7266ca5b",
   "metadata": {},
   "source": [
    "# (2) Write the training set that results after the application of the preprocessing formula below.\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4afc546-b483-4be1-b5e3-e1fcffbdfb8d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "544c8007-8b19-4621-b441-d6cb28829caa",
   "metadata": {},
   "source": [
    "# (3) Explain your observation whether this data preprocessing alleviates the scale problem.\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2fef4683-fd6a-400b-a8a8-4dd487a8092f",
   "metadata": {},
   "source": [
    "#Answer:-\n",
    "Whether it alleviates the scale problem\n",
    "Yes — after standardization, all features have comparable magnitude (~ zero mean, unit variance), so weight updates treat each feature more equally, improving convergence.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c2eff41-4b5a-4423-941b-bb5aafc54802",
   "metadata": {},
   "source": [
    "    (1) Assuming that the weight vector of the perceptron is and the bias is 0, explain the scale problem with the training set. \n",
    "    \n",
    "    (2) Write the training set that results after the application of the preprocessing formula below. ​(5.9)\n",
    "    \n",
    "    (3) Explain your observation whether this data preprocessing alleviates the scale problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ee10008-35ef-48be-b516-1d6ba4ffc29a",
   "metadata": {},
   "source": [
    "    (1) Scale problem\n",
    "Blood pressure, height, and weight have different numerical ranges. Since the perceptron bias is 0, features with larger values dominate the dot product, causing unequal influence of features. This is called the scale problem.\n",
    "\n",
    "    (2) Training set after preprocessing\n",
    "After applying preprocessing formula (5.9), each feature is normalized/scaled, so blood pressure, height, and weight are transformed into comparable values (e.g., between 0 and 1 or with mean 0).\n",
    "\n",
    "    (3) Observation\n",
    "Yes, preprocessing alleviates the scale problem by bringing all features to the same scale, allowing each feature to contribute fairly and improving perceptron learning."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b20c0aa7-e9d6-4f16-992f-d0a5c5cd0c65",
   "metadata": {},
   "source": [
    "# 3. Neural networks, convolution layers are repeated multiple times in deep learning, and it causes some nodes get omitted to a great degree. What technique can you use to prevent this problem?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7edd841d-54c8-4fd4-a6cc-719290c2fa3e",
   "metadata": {},
   "source": [
    "#Answer:- \n",
    "5. Problem: Repeating convolution layers in deep neural networks can cause some nodes to become inactive or omitted, known as the vanishing/exploding activation problem.\n",
    "Solution: Use Dropout — a technique that randomly “drops” nodes during training, which:\n",
    "Prevents overfitting\n",
    "Ensures all nodes learn useful features\n",
    "Improves generalization\n",
    "Other techniques like Batch Normalization also help stabilize training."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe676a01-be8c-470d-85eb-711b0152152b",
   "metadata": {},
   "source": [
    "# 4. Weight initialization should generate random numbers in the range [-1,1]. Please provide the Python code that does this function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a194b8ef-2084-4110-9dee-c77e1a99aa17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.73732152 -0.17998871 -0.48819784 -0.84456052  0.44468135]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Example: initialize weight vector of size 5\n",
    "weights = np.random.uniform(-1, 1, size=5)\n",
    "print(weights)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09ec74e6-1d20-4df3-abb4-ba7c39b0f019",
   "metadata": {},
   "source": [
    "# 5. You want to train a classifier when you have many unlabeled training data but only a few thousand labeled data. Describe how the autoencoder can be helpful and how to work."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a3f8e83-23bd-42c7-a3af-dc951d0891d5",
   "metadata": {},
   "source": [
    "Scenario: Many unlabeled data + few labeled data.\n",
    "\n",
    "      How autoencoder helps:\n",
    "     1.Train an autoencoder on all unlabeled data to learn a compressed representation (features) of the input.\n",
    "     2.Use the encoder part to extract meaningful features.\n",
    "     3.Train a classifier using the few labeled data on these learned features.\n",
    "\n",
    "     \n",
    "         Benefit:\n",
    "    1.Leverages unsupervised learning to capture patterns in unlabeled data\n",
    "    2.Reduces dependency on large labeled datasets\n",
    "    3.Improves classifier performance with limited labeled data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9414e822-0fe3-43be-ac3d-17bac061099e",
   "metadata": {},
   "source": [
    "# 6.Train the 'Fashion-mnist' dataset classification model with reference to the following code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d2b4eda-502f-4ede-8312-6c2bb97881b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from tensorflow.keras.datasets.fashion_mnist import load_data\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "\n",
    "# Load data\n",
    "(X_train, y_train), (X_test, y_test) = load_data()\n",
    "\n",
    "# Data preprocessing\n",
    "X_train = X_train.reshape((-1, 28, 28, 1)) / 255.0\n",
    "X_test = X_test.reshape((-1, 28, 28, 1)) / 255.0\n",
    "\n",
    "# Convert labels to one-hot encoding\n",
    "y_train = to_categorical(y_train, 10)\n",
    "y_test = to_categorical(y_test, 10)\n",
    "\n",
    "# Training parameters\n",
    "batch_size = 8\n",
    "n_epochs = 20\n",
    "learn_rate = 0.0001\n",
    "\n",
    "# CNN Model\n",
    "model = tf.keras.Sequential([\n",
    "    tf.keras.Input(shape=(28, 28, 1)),\n",
    "    \n",
    "    # Convolutional layer 1\n",
    "    tf.keras.layers.Conv2D(32, kernel_size=(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    # Convolutional layer 2\n",
    "    tf.keras.layers.Conv2D(64, kernel_size=(3,3), activation='relu'),\n",
    "    tf.keras.layers.MaxPooling2D(pool_size=(2,2)),\n",
    "    \n",
    "    # Flatten and fully connected layers\n",
    "    tf.keras.layers.Flatten(),\n",
    "    tf.keras.layers.Dense(128, activation='relu'),\n",
    "    tf.keras.layers.Dropout(0.5),\n",
    "    tf.keras.layers.Dense(10, activation='softmax')  # 10 classes\n",
    "])\n",
    "\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=learn_rate),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    batch_size=batch_size,\n",
    "    epochs=n_epochs,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "# Evaluate the model\n",
    "test_loss, test_acc = model.evaluate(X_test, y_test, verbose=0)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
